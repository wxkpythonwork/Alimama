{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from matplotlib.pylab import rcParams\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "# rcParams['figure.figsize'] = 12, 8\n",
    "#rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_brand_id</th>\n",
       "      <th>item_city_id</th>\n",
       "      <th>item_price_level</th>\n",
       "      <th>item_sales_level</th>\n",
       "      <th>item_collected_level</th>\n",
       "      <th>item_pv_level</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_gender_id</th>\n",
       "      <th>...</th>\n",
       "      <th>context_timestamp</th>\n",
       "      <th>context_page_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>shop_review_num_level</th>\n",
       "      <th>shop_review_positive_rate</th>\n",
       "      <th>shop_star_level</th>\n",
       "      <th>shop_score_service</th>\n",
       "      <th>shop_score_delivery</th>\n",
       "      <th>shop_score_description</th>\n",
       "      <th>is_trade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>4.781110e+05</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "      <td>478111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.613058e+18</td>\n",
       "      <td>4.594616e+18</td>\n",
       "      <td>4.838454e+18</td>\n",
       "      <td>5.500479e+18</td>\n",
       "      <td>6.777242</td>\n",
       "      <td>11.133741</td>\n",
       "      <td>12.199288</td>\n",
       "      <td>17.128648</td>\n",
       "      <td>4.609870e+18</td>\n",
       "      <td>0.213045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537502e+09</td>\n",
       "      <td>4003.362343</td>\n",
       "      <td>4.733809e+18</td>\n",
       "      <td>15.768296</td>\n",
       "      <td>0.994830</td>\n",
       "      <td>5013.175187</td>\n",
       "      <td>0.971124</td>\n",
       "      <td>0.970497</td>\n",
       "      <td>0.974863</td>\n",
       "      <td>0.018812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.663453e+18</td>\n",
       "      <td>2.710124e+18</td>\n",
       "      <td>2.694774e+18</td>\n",
       "      <td>2.464859e+18</td>\n",
       "      <td>1.088979</td>\n",
       "      <td>2.588877</td>\n",
       "      <td>2.497317</td>\n",
       "      <td>2.160771</td>\n",
       "      <td>2.663365e+18</td>\n",
       "      <td>0.514703</td>\n",
       "      <td>...</td>\n",
       "      <td>1.726216e+05</td>\n",
       "      <td>4.019755</td>\n",
       "      <td>2.596096e+18</td>\n",
       "      <td>2.825281</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>2.590979</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>0.025025</td>\n",
       "      <td>0.135859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.823219e+12</td>\n",
       "      <td>6.964907e+14</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.477979e+13</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537200e+09</td>\n",
       "      <td>4001.000000</td>\n",
       "      <td>1.543560e+15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4999.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.308539e+18</td>\n",
       "      <td>2.251088e+18</td>\n",
       "      <td>2.357806e+18</td>\n",
       "      <td>3.948283e+18</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.302746e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537351e+09</td>\n",
       "      <td>4001.000000</td>\n",
       "      <td>2.498543e+18</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.992779</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>0.966360</td>\n",
       "      <td>0.965684</td>\n",
       "      <td>0.969268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.613362e+18</td>\n",
       "      <td>4.636270e+18</td>\n",
       "      <td>5.051040e+18</td>\n",
       "      <td>6.738284e+18</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.608564e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537506e+09</td>\n",
       "      <td>4001.000000</td>\n",
       "      <td>4.629590e+18</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5013.000000</td>\n",
       "      <td>0.972347</td>\n",
       "      <td>0.971590</td>\n",
       "      <td>0.978493</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.920942e+18</td>\n",
       "      <td>7.007158e+18</td>\n",
       "      <td>7.565625e+18</td>\n",
       "      <td>7.534239e+18</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.914401e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537635e+09</td>\n",
       "      <td>4004.000000</td>\n",
       "      <td>6.740687e+18</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5015.000000</td>\n",
       "      <td>0.977822</td>\n",
       "      <td>0.976978</td>\n",
       "      <td>0.983640</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.223362e+18</td>\n",
       "      <td>9.221980e+18</td>\n",
       "      <td>9.222396e+18</td>\n",
       "      <td>9.151271e+18</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.223345e+18</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537805e+09</td>\n",
       "      <td>4020.000000</td>\n",
       "      <td>9.220773e+18</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5020.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        instance_id       item_id  item_brand_id  item_city_id  \\\n",
       "count  4.781110e+05  4.781110e+05   4.781110e+05  4.781110e+05   \n",
       "mean   4.613058e+18  4.594616e+18   4.838454e+18  5.500479e+18   \n",
       "std    2.663453e+18  2.710124e+18   2.694774e+18  2.464859e+18   \n",
       "min    3.823219e+12  6.964907e+14  -1.000000e+00 -1.000000e+00   \n",
       "25%    2.308539e+18  2.251088e+18   2.357806e+18  3.948283e+18   \n",
       "50%    4.613362e+18  4.636270e+18   5.051040e+18  6.738284e+18   \n",
       "75%    6.920942e+18  7.007158e+18   7.565625e+18  7.534239e+18   \n",
       "max    9.223362e+18  9.221980e+18   9.222396e+18  9.151271e+18   \n",
       "\n",
       "       item_price_level  item_sales_level  item_collected_level  \\\n",
       "count     478111.000000     478111.000000         478111.000000   \n",
       "mean           6.777242         11.133741             12.199288   \n",
       "std            1.088979          2.588877              2.497317   \n",
       "min            0.000000         -1.000000              0.000000   \n",
       "25%            6.000000         10.000000             11.000000   \n",
       "50%            7.000000         11.000000             12.000000   \n",
       "75%            8.000000         13.000000             14.000000   \n",
       "max           17.000000         17.000000             17.000000   \n",
       "\n",
       "       item_pv_level       user_id  user_gender_id      ...        \\\n",
       "count  478111.000000  4.781110e+05   478111.000000      ...         \n",
       "mean       17.128648  4.609870e+18        0.213045      ...         \n",
       "std         2.160771  2.663365e+18        0.514703      ...         \n",
       "min         0.000000  2.477979e+13       -1.000000      ...         \n",
       "25%        16.000000  2.302746e+18        0.000000      ...         \n",
       "50%        17.000000  4.608564e+18        0.000000      ...         \n",
       "75%        19.000000  6.914401e+18        0.000000      ...         \n",
       "max        21.000000  9.223345e+18        2.000000      ...         \n",
       "\n",
       "       context_timestamp  context_page_id       shop_id  \\\n",
       "count       4.781110e+05    478111.000000  4.781110e+05   \n",
       "mean        1.537502e+09      4003.362343  4.733809e+18   \n",
       "std         1.726216e+05         4.019755  2.596096e+18   \n",
       "min         1.537200e+09      4001.000000  1.543560e+15   \n",
       "25%         1.537351e+09      4001.000000  2.498543e+18   \n",
       "50%         1.537506e+09      4001.000000  4.629590e+18   \n",
       "75%         1.537635e+09      4004.000000  6.740687e+18   \n",
       "max         1.537805e+09      4020.000000  9.220773e+18   \n",
       "\n",
       "       shop_review_num_level  shop_review_positive_rate  shop_star_level  \\\n",
       "count          478111.000000              478111.000000    478111.000000   \n",
       "mean               15.768296                   0.994830      5013.175187   \n",
       "std                 2.825281                   0.011972         2.590979   \n",
       "min                 0.000000                  -1.000000      4999.000000   \n",
       "25%                14.000000                   0.992779      5012.000000   \n",
       "50%                16.000000                   1.000000      5013.000000   \n",
       "75%                17.000000                   1.000000      5015.000000   \n",
       "max                25.000000                   1.000000      5020.000000   \n",
       "\n",
       "       shop_score_service  shop_score_delivery  shop_score_description  \\\n",
       "count       478111.000000        478111.000000           478111.000000   \n",
       "mean             0.971124             0.970497                0.974863   \n",
       "std              0.023818             0.023777                0.025025   \n",
       "min             -1.000000            -1.000000               -1.000000   \n",
       "25%              0.966360             0.965684                0.969268   \n",
       "50%              0.972347             0.971590                0.978493   \n",
       "75%              0.977822             0.976978                0.983640   \n",
       "max              1.000000             1.000000                1.000000   \n",
       "\n",
       "            is_trade  \n",
       "count  478111.000000  \n",
       "mean        0.018812  \n",
       "std         0.135859  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"round1_ijcai_18_train_20180301.txt\", sep=\" \")\n",
    "test_data = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=\" \")\n",
    "#print(train_data.head())\n",
    "#看是否有缺失值\n",
    "train_data.drop_duplicates(inplace=True)#默认为所有行相同去重，也可以指定列下去重\n",
    "train_data.isnull().any().any()\n",
    "test_data.isnull().any().any()\n",
    "#训练数据--测试数据 是否信息类目一致\n",
    "#diff = train_columns.difference(test_cloumns)\n",
    "#print(diff)\n",
    "train_columns = set(train_data.columns)\n",
    "#print(train_columns)\n",
    "test_cloumns = set(test_data.columns)\n",
    "diff = train_columns - test_cloumns\n",
    "#是否有字符串，有可以做LabelEncode\n",
    "\n",
    "train_data.describe()\n",
    "#负样本8994 << 正样本469117 =50倍(样本不平衡问题)https://www.leiphone.com/news/201706/dTRE5ow9qBVLkZSY.html\n",
    "\n",
    "#print(train_data['is_trade'].value_counts())\n",
    "#print(train_data.loc[:,'is_trade'].value_counts()\n",
    "#统计信息绘图\n",
    "# dis = train_data['is_trade'].value_counts()\n",
    "# plt.figure(figsize = (20, 8))\n",
    "# sns.barplot(dis.index, dis.values, alpha=0.7, color='red')\n",
    "# plt.xlabel('value', fontsize=10)\n",
    "# plt.ylabel('number', fontsize=10)\n",
    "# plt.show()\n",
    "# print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "test在train中的个数为:5173\n",
      "loading success ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['user_gender_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0831616170653\n",
      "耗时： 11.616224527359009\n",
      "model-1 over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def timestamp_convert(value):\n",
    "    format = '%Y-%m-%d %H:%M:%S'#要转化的格式\n",
    "    value = time.localtime(value)\n",
    "    #print(value)\n",
    "    newday = time.strftime(format, value)\n",
    "    return newday\n",
    "\n",
    "def covert_seconds_to_days(data):\n",
    "    #frame['days'] = (pd.to_datetime(frame['date']) - parse('2015-01-01')).dt.days#转化为时间\n",
    "    data['date'] = data['context_timestamp'].apply(timestamp_convert)#将这一列的每个数据进行一一转换\n",
    "    data['day'] = data.date.apply(lambda x: int(x[8:10]))#将这一列的每个数据进行一一切片\n",
    "    data['hour'] = data.date.apply(lambda x: int(x[11:13]))\n",
    "#     user_query_day = data.groupby(['user_id', 'day']).size().reset_index().rename(columns={0:'user_query_day'})\n",
    "#      #默认inner。left、right分别保留左边、右边部分数据（缺失补NAN）,inner公共部分,outer俩边所有\n",
    "#     data = pd.merge(data, user_query_day, how='inner', on=['user_id','user_query_day'])\n",
    "#     user_query_day_hour = data.groupby(['user_id', 'day', 'hour']).size().reset_index().rename(columns={1:\"user_query_day_hour\"})\n",
    "#     data = pd.merge(data, user_query_day_hour, how='inner', on=['user_id', 'day', 'hour'])\n",
    "    del data['context_timestamp']\n",
    "    return data\n",
    "\n",
    "def lgb_baselinemodel(train_data,test_data,online):\n",
    "    features = ['context_id',\n",
    "             'context_page_id',\n",
    "             'instance_id',         \n",
    "             'item_brand_id',\n",
    "             'item_city_id',\n",
    "             'item_collected_level',\n",
    "             'item_id',\n",
    "             'item_price_level',\n",
    "             'item_pv_level',\n",
    "             'shop_id',\n",
    "             'shop_review_positive_rate',\n",
    "             'shop_score_delivery',\n",
    "             'shop_score_description',\n",
    "             'shop_star_level',\n",
    "             'user_age_level',\n",
    "             'user_gender_id',\n",
    "             'user_id',\n",
    "             'day',\n",
    "             'hour']\n",
    "    labels = 'is_trade'\n",
    "    if online == False:\n",
    "        training_data = train_data[(train_data['day'] < 24) & (train_data['day'] >= 18)]\n",
    "        #print(train_data)\n",
    "        train_test_data = train_data[train_data['day'] == 24]\n",
    "#        print(train_test_data)\n",
    "        #feature_test = train_test_data.columns.tolist().remove('is_trade')\n",
    "        #drop默认删除行axis=0，返回一个新的DF可以不改变原有数据，inplace=True时.drop()执行内部删除，不返回任何值，原数据发生改变\n",
    "        #lgb_cv(train_data.drop['label', axis=1， inplace=True], train_data.iloc[:, ['label']])\n",
    "        clf = lgb.LGBMClassifier(num_leaves=64, max_depth=8, n_estimators=100, n_jobs=20)\n",
    "        clf.fit(training_data[features], training_data[labels], feature_name=features, categorical_feature=['user_gender_id'])\n",
    "        train_test_data['pred'] = clf.predict_proba(train_test_data[features],)[:, 1]\n",
    "#        print(train_test_data['pred'])\n",
    "        logloss = log_loss(train_test_data['is_trade'],train_test_data['pred'])\n",
    "        print(logloss)\n",
    "        \n",
    "    if online == True:\n",
    "        train_data = train_data.copy()\n",
    "        test_data = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=\" \")\n",
    "        test_data.drop_duplicates(inplace=True)\n",
    "        test_data = covert_seconds_to_days(test_data)\n",
    "        model = lgb.LGBMClassifier(num_leaves=64, max_depth=7, n_estimators=80, n_jobs=20)\n",
    "        model.fit(train_data[features], train_data[labels], feature_name=features, categorical_feature=['user_gender_id'])\n",
    "        test_data['proba'] = model.predict_proba(test_data[features],)[:, 1]\n",
    "        test_data[['instance_id','proba']].to_csv('baseline1.csv', index=False, sep=\" \")\n",
    "\n",
    "def lgb_gridsearch_model(train_data):\n",
    "    train_data.drop_duplicates(inplace=True)#默认为所有行相同去重，也可以指定列下去重\n",
    "    train_data = covert_seconds_to_days(train_data)\n",
    "\n",
    "    d_x = train_data.loc[:,features]\n",
    "    d_y = train_data.loc[:,label]\n",
    "#     train_X, test_X, train_y, test_y = train_test_split(\n",
    "#             d_x, d_y, testsize = 0.3, random_state = 20)\n",
    "    print (\"create classifier ...\")\n",
    "    param_grid = {\n",
    "        \"learing rate\": [0.05, 0.06, 0.07, 0.08, 0.1],\n",
    "        \"n_estimators\": [50, 60, 70],\n",
    "        \"max_depth\": [5, 6, 7, 8]\n",
    "    }\n",
    "    params = {\n",
    "        \"objective\": 'binary',#二分类;multiclass多分类\n",
    "        \"metric\": 'multi_logloss',\n",
    "        \"max_bin\": 255,\n",
    "        \"max_depth\": 7,\n",
    "        \"learning_rate\": 0.25,\n",
    "        \"n_estimators\": 80\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt', objective='binary', nthread=8,seed=20)\n",
    "    model.n_class = 2\n",
    "    print(\"running grid search ...\")\n",
    "    print(\"=\" * 50, '\\n')\n",
    "    searcher = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "    searcher.fit(d_x, d_y)\n",
    "    print(searcher.grid_scores_)\n",
    "    print(\"=\" * 50, '\\n')\n",
    "    print(searcher.best_params_)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    online = False\n",
    "    print('loading data ...')\n",
    "    train_data = pd.read_csv(\"round1_ijcai_18_train_20180301.txt\", sep=\" \")\n",
    "    test_data = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=\" \")\n",
    "    is_test_in_train = test_data.user_id.isin(train_data.user_id)\n",
    "    print(\"test在train中的个数为:{}\".format(is_test_in_train.sum()))\n",
    "    print(\"loading success ...\")\n",
    "    #缺失值用中位数替换\n",
    "    none_list = ['item_brand_id','item_city_id','item_sales_level','user_gender_id','shop_review_positive_rate','shop_score_service','shop_score_delivery','shop_score_description']\n",
    "    for label in none_list:\n",
    "        test_data = test_data.replace({label: -1},test_data[label].mean())#mean()\n",
    "        train_data = train_data.replace({label:-1},train_data[label].mean())\n",
    "    train_data.drop_duplicates(inplace=True)#默认为所有行相同去重，也可以指定列下去重\n",
    "    train_data = covert_seconds_to_days(train_data)\n",
    "    #print(train_data.columns)\n",
    "    #print(train_data.day.value_counts())\n",
    "    \n",
    "    features = ['context_id',\n",
    "             'context_page_id',\n",
    "             'instance_id',\n",
    "             'is_trade',\n",
    "             'item_brand_id',             \n",
    "             'item_city_id',\n",
    "             'item_collected_level',\n",
    "             'item_id',\n",
    "             'item_price_level',\n",
    "             'item_pv_level',\n",
    "             'shop_id',\n",
    "             'shop_review_positive_rate',\n",
    "             'shop_score_delivery',\n",
    "             'shop_score_description',\n",
    "             'shop_star_level',\n",
    "             'user_age_level',\n",
    "             'user_gender_id',\n",
    "             'user_id',\n",
    "             'day',\n",
    "             'hour']\n",
    "    labels = 'is_trade'\n",
    "    lgb_baselinemodel(train_data,test_data,online)\n",
    "    end1 = time.time()\n",
    "    print(\"耗时：\", (end1 - start))\n",
    "    print('model-1 over')\n",
    "    \n",
    "#     lgb_gridsearch_model(train_data)\n",
    "#     end2 = time.time()\n",
    "#     print(\"耗时:\", (end2 - start))\n",
    "#     print('model-2 over')\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "loading success ...\n",
      "dealing with NAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss value：0.08310006154955947\n",
      "耗时：15.208750009536743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "\n",
    "def timestamp_convert(value):\n",
    "    format = '%Y-%m-%d %H:%M:%S'  # 要转化的格式\n",
    "    value = time.localtime(value)\n",
    "    # print(value)\n",
    "    newdata = time.strftime(format, value)\n",
    "    return newdata\n",
    "\n",
    "\n",
    "def covert_seconds_to_days(train_data):\n",
    "    # frame['days'] = (pd.to_datetime(frame['date']) - parse('2015-01-01')).dt.days#转化为时间\n",
    "    train_data['date'] = train_data['context_timestamp'].apply(timestamp_convert)  # 将这一列的每个数据进行一一转换\n",
    "    train_data['day'] = train_data.date.apply(lambda x: int(x[8:10]))  # 将这一列的每个数据进行一一切片\n",
    "    train_data['hour'] = train_data.date.apply(lambda x: int(x[11:13]))\n",
    "    #     user_query_day = data.groupby(['user_id', 'day']).size().reset_index().rename(columns={0:'user_query_day'})\n",
    "    #      #默认inner。left、right分别保留左边、右边部分数据（缺失补NAN）,inner公共部分,outer俩边所有\n",
    "    #     data = pd.merge(data, user_query_day, how='inner', on=['user_id','user_query_day'])\n",
    "    #     user_query_day_hour = data.groupby(['user_id', 'day', 'hour']).size().reset_index().rename(columns={1:\"user_query_day_hour\"})\n",
    "    #     data = pd.merge(data, user_query_day_hour, how='inner', on=['user_id', 'day', 'hour']\n",
    "    return train_data\n",
    "\n",
    "def lgb_baselinemodel(train_data, test_data, online):\n",
    "\n",
    "    #print(train_data.columns.tolist())#去掉  is_trade\n",
    "    features = ['instance_id', 'item_id', 'item_brand_id', 'item_city_id', 'user_id', 'user_age_level',\n",
    "                'user_occupation_id', 'user_star_level', 'context_id', 'context_page_id', 'shop_id',\n",
    "                'shop_review_num_level', 'day', 'hour', 'gender_class_0', 'gender_class_2',\n",
    "                'item_pv_sales', 'item_pv_collected', 'item_total_sales', 'shop_score', 'shop_view']\n",
    "\n",
    "    label = ['is_trade']\n",
    "\n",
    "    if online == False:\n",
    "        training_data = train_data[(train_data['day'] < 24) & (train_data['day'] >= 18)]\n",
    "        train_test_data = train_data[train_data['day'] == 24]\n",
    "        clf = lgb.LGBMClassifier(num_leaves=64, max_depth=8, n_estimators=100, n_jobs=20)\n",
    "        clf.fit(training_data[features], training_data[label], feature_name=features)\n",
    "        train_test_data['pred'] = clf.predict_proba(train_test_data[features], )[:, 1]\n",
    "        logloss = log_loss(train_test_data['is_trade'], train_test_data['pred'])\n",
    "        print('logloss value：{}'.format(logloss))\n",
    "\n",
    "\n",
    "# if online == True:\n",
    "#        train_data = train_data.copy()\n",
    "#        test_data = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=\" \")\n",
    "#        test_data.drop_duplicates(inplace=True)\n",
    "#        test_data = covert_seconds_to_days(test_data)\n",
    "#        model = lgb.LGBMClassifier(num_leaves=64, max_depth=7, n_estimators=80, n_jobs=20)\n",
    "#        model.fit(train_data[features], train_data[label], feature_name=features)\n",
    "#        test_data['proba'] = model.predict_proba(test_data[features],)[:, 1]\n",
    "#        test_data[['instance_id','proba']].to_csv('baseline_lgb.csv', index=False, sep=\" \")\n",
    "\n",
    "def main():\n",
    "    print('loading data ...')\n",
    "    train_data = pd.read_csv(\"round1_ijcai_18_train_20180301.txt\", sep=\" \")\n",
    "    test_data = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=\" \")\n",
    "    print(\"loading success ...\")\n",
    "    print(\"dealing with NAN\")\n",
    "    nan_list = ['item_brand_id', 'item_city_id', 'item_sales_level', 'user_age_level', 'user_occupation_id',\n",
    "                'user_star_level',\n",
    "                'user_gender_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery',\n",
    "                'shop_score_description']\n",
    "    # 缺失值-1替换为中位数\n",
    "    for label in nan_list:\n",
    "        train_data = train_data.replace({label: -1}, train_data[label].median())  # mean()\n",
    "        train_data = train_data.replace({label: -1}, train_data[label].median())\n",
    "    for label in nan_list:\n",
    "        test_data = test_data.replace({label: -1}, test_data[label].median())  # mean()\n",
    "        test_data = test_data.replace({label: -1}, test_data[label].median())\n",
    "\n",
    "    train_data.drop_duplicates(inplace=True)  # 默认为所有行相同去重，也可以指定列下去重\n",
    "    train_data = covert_seconds_to_days(train_data)\n",
    "    train_data.drop(['date', 'context_timestamp'], axis=1, inplace=True)\n",
    "    # 先不分析属性从属关系\n",
    "    train_data.drop(['item_category_list', 'item_property_list', 'predict_category_property'], axis=1, inplace=True)\n",
    "\n",
    "    # 归一化\n",
    "    # train_data.context_page_id = train_data.context_page_id.values - 4000\n",
    "    # train_data.user_age_level = train_data.user_age_level.values - 1000\n",
    "    train_data.context_page_id = (train_data['context_page_id'] - train_data['context_page_id'].min())/(train_data['context_page_id'].max() - train_data['context_page_id'].min())\n",
    "    train_data.user_age_level = (train_data['user_age_level'] - train_data['user_age_level'].min())/(train_data['user_age_level'].max() - train_data[    'user_age_level'].min())\n",
    "    # 将user_gender_id属性分割成多个(适合比较少类目的属性)\n",
    "    gender_id_class = pd.get_dummies(train_data.user_gender_id)\n",
    "    gender_class = gender_id_class.rename(columns=lambda x: 'gender_class_' + str(x))\n",
    "    train_data = pd.concat([train_data, gender_class], axis=1)\n",
    "    train_data.drop(['user_gender_id'], axis=1, inplace=True)\n",
    "\n",
    "    #item内构建新特征\n",
    "    train_data['item_pv_sales'] = train_data['item_pv_level'] / train_data['item_sales_level']\n",
    "    train_data['item_pv_collected'] = train_data['item_pv_level'] / train_data['item_collected_level']\n",
    "    train_data['item_total_sales'] = train_data['item_price_level'] * train_data['item_sales_level']\n",
    "    train_data.drop(['item_pv_level','item_sales_level','item_collected_level','item_price_level'],\n",
    "                    axis=1,inplace=True)\n",
    "\n",
    "    #shop内构建特征\n",
    "    train_data['shop_score'] = (train_data.shop_score_service * train_data.shop_score_delivery * train_data.shop_score_description) / 3\n",
    "    train_data['shop_view'] = train_data.shop_review_positive_rate * train_data.shop_star_level\n",
    "    train_data.drop(['shop_score_service', 'shop_score_delivery', 'shop_score_description'],\n",
    "                    axis=1, inplace=True)\n",
    "    train_data.drop(['shop_review_positive_rate','shop_star_level'],\n",
    "                    axis=1,inplace=True)\n",
    "#     dfdata = train_data.corr()   \n",
    "#     plt.subplots(figsize=(30,30))\n",
    "#     #vmax,vmin:分别是热力图的颜色取值最大和最小范围\n",
    "#     #annot(annotate的缩写):默认取值False；如果是True，在热力图每个方格写入数据\n",
    "#     sns.heatmap(dfdata, annot=True, vmax=1, square=True, cmap='Reds')\n",
    "#     plt.savefig(\"./features_corr.png\")\n",
    "#     plt.show()\n",
    "\n",
    "    lgb_baselinemodel(train_data, test_data, online)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    online = False\n",
    "    main()\n",
    "    print(\"耗时：{}\".format((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
